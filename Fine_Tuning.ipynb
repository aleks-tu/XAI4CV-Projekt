{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODysF5ZrfOF4XSITbNUz/y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleks-tu/XAI4CV-Projekt/blob/main/Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Base: https://medium.com/@engr.akhtar.awan/how-to-fine-tune-the-resnet-50-model-on-your-target-dataset-using-pytorch-187abdb9beeb\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "-SXoTPh3Q_GO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Download Dataset\n",
        "# ----------------------------\n",
        "\n",
        "# Dataset page: https://www.kaggle.com/datasets/kritikseth/fruit-and-vegetable-image-recognition?resource=download\n",
        "# 36 classes\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kritikseth/fruit-and-vegetable-image-recognition\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTu6pysyQtrL",
        "outputId": "c09c66fa-63d4-45e0-d1a0-abbfe11d6b63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kritikseth/fruit-and-vegetable-image-recognition?dataset_version_number=8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.98G/1.98G [00:51<00:00, 41.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/kritikseth/fruit-and-vegetable-image-recognition/versions/8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "17X-Z7BvMh4L"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Data Preparation\n",
        "# ----------------------------\n",
        "\n",
        "# Define the transformation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(\n",
        "#         mean=[0.485, 0.456, 0.406],\n",
        "#         std=[0.229, 0.224, 0.225]\n",
        "#     )\n",
        "# ])\n",
        "\n",
        "# See https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
        "# Load transformations:\n",
        "weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "transform = weights.transforms()\n",
        "\n",
        "# Define a specific image loader to deal with images containing transparent pixels (Make them white)\n",
        "from PIL import Image\n",
        "from typing import Union\n",
        "from pathlib import Path\n",
        "# def rgba_pil_loader(path: Union[str, Path]) -> Image.Image:\n",
        "#   # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "\n",
        "#   # Some images are corrupt\n",
        "#   corrupt_images_replacement = {\n",
        "#       \"/kaggle/input/fruit-and-vegetable-image-recognition/train/bell pepper/Image_56.jpg\": \"/kaggle/input/fruit-and-vegetable-image-recognition/train/bell pepper/Image_55.jpg\",\n",
        "#       \"/kaggle/input/fruit-and-vegetable-image-recognition/train/potato/Image_69.png\": \"/kaggle/input/fruit-and-vegetable-image-recognition/train/potato/Image_68.png\",\n",
        "#       \"/kaggle/input/fruit-and-vegetable-image-recognition/train/carrot/Image_68.png\": \"/kaggle/input/fruit-and-vegetable-image-recognition/train/carrot/Image_67.png\",\n",
        "#       \"/kaggle/input/fruit-and-vegetable-image-recognition/train/soy beans/Image_13.png\": \"/kaggle/input/fruit-and-vegetable-image-recognition/train/soy beans/Image_12.png\",\n",
        "#       \"/kaggle/input/fruit-and-vegetable-image-recognition/train/paprika/Image_26.png\": \"/kaggle/input/fruit-and-vegetable-image-recognition/train/paprika/Image_25.png\",\n",
        "#   }\n",
        "\n",
        "#   with open(path, \"rb\") as f:\n",
        "#       img = Image.open(f)\n",
        "#       if (img.mode in (\"RGBA\", \"LA\") or (img.mode == \"P\" and \"transparency\" in img.info)) and len(img.split())>3:\n",
        "#       #   print(\"TRANSPARENT!\")\n",
        "#       #   alpha = img.convert(\"RGBA\").split()[-1]\n",
        "#       #   print(\"Converted\")\n",
        "#       #   bg = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
        "#       #   print(\"made bg\")\n",
        "#       #   bg.paste(img, mask=alpha)\n",
        "#       #   print(\"pasted img\")\n",
        "#       #   return bg\n",
        "\n",
        "#         # print(\"TRANSPARENT\")\n",
        "#         # img = img.convert(\"RGBA\")\n",
        "#         # datas = img.getdata()\n",
        "#         # newData = []\n",
        "#         # for item in datas:\n",
        "#         #     if item[0] == 255 and item[1] == 255 and item[2] == 255:\n",
        "#         #         newData.append((255, 255, 255, 0))\n",
        "#         #     else:\n",
        "#         #         newData.append(item)\n",
        "#         # img.putdata(newData)\n",
        "\n",
        "#         background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
        "#         # print(\"here\")\n",
        "#         print(f\"length: {len(img.split())}\")\n",
        "#         background.paste(img, mask=img.split()[3]) # 3 is the alpha channel\n",
        "\n",
        "#         # Print value of image at pixel 50, 50:\n",
        "#         # print(img.getpixel((50, 50)))\n",
        "\n",
        "#         # print(\"RETURNING MODIFIED IMG\")\n",
        "#         return img.convert('RGB')\n",
        "#       elif len(img.split())<3:\n",
        "#         print(\"CORRUPT IMAGE -------------------------\")\n",
        "#         print(path)\n",
        "#         return Image.new(\"RGB\", img.size, (255,255,255))\n",
        "#       else:\n",
        "#         # print(\"NOT TRANSPARENT\")\n",
        "#         return img.convert(\"RGB\")  # Ensure all images are RGB\n",
        "\n",
        "\n",
        "\n",
        "# # Load the data\n",
        "# train_data = torchvision.datasets.ImageFolder(root=\"/kaggle/input/fruit-and-vegetable-image-recognition/train\", loader=rgba_pil_loader, transform=transform)\n",
        "# test_data = torchvision.datasets.ImageFolder(root=\"/kaggle/input/fruit-and-vegetable-image-recognition/validation\", loader=rgba_pil_loader, transform=transform)\n",
        "\n",
        "# Load the data\n",
        "train_data = torchvision.datasets.ImageFolder(root=path+\"/train\", transform=transform)\n",
        "test_data = torchvision.datasets.ImageFolder(root=path+\"/validation\", transform=transform)\n",
        "\n",
        "# Define the dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Modify Model\n",
        "# ----------------------------\n",
        "\n",
        "# Define the model\n",
        "model = resnet50(weights=weights)\n",
        "\n",
        "# # Freeze all parameters:\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Freeze all layers but the third and fourth (last two) sequential blocks\n",
        "for name, param in model.named_parameters():\n",
        "  if not ('layer4' in name or 'layer3' in name):\n",
        "      param.requires_grad = False\n",
        "  print(f\"Training {name}: {param.requires_grad}\")\n",
        "\n",
        "# Replace the last layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(train_data.classes))\n",
        "print(f\"Number of classes: {train_data.classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILAEz9pwVkv3",
        "outputId": "b6d1e75e-19a6-4a41-fcd6-b0f48b7fe8eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training conv1.weight: False\n",
            "Training bn1.weight: False\n",
            "Training bn1.bias: False\n",
            "Training layer1.0.conv1.weight: False\n",
            "Training layer1.0.bn1.weight: False\n",
            "Training layer1.0.bn1.bias: False\n",
            "Training layer1.0.conv2.weight: False\n",
            "Training layer1.0.bn2.weight: False\n",
            "Training layer1.0.bn2.bias: False\n",
            "Training layer1.0.conv3.weight: False\n",
            "Training layer1.0.bn3.weight: False\n",
            "Training layer1.0.bn3.bias: False\n",
            "Training layer1.0.downsample.0.weight: False\n",
            "Training layer1.0.downsample.1.weight: False\n",
            "Training layer1.0.downsample.1.bias: False\n",
            "Training layer1.1.conv1.weight: False\n",
            "Training layer1.1.bn1.weight: False\n",
            "Training layer1.1.bn1.bias: False\n",
            "Training layer1.1.conv2.weight: False\n",
            "Training layer1.1.bn2.weight: False\n",
            "Training layer1.1.bn2.bias: False\n",
            "Training layer1.1.conv3.weight: False\n",
            "Training layer1.1.bn3.weight: False\n",
            "Training layer1.1.bn3.bias: False\n",
            "Training layer1.2.conv1.weight: False\n",
            "Training layer1.2.bn1.weight: False\n",
            "Training layer1.2.bn1.bias: False\n",
            "Training layer1.2.conv2.weight: False\n",
            "Training layer1.2.bn2.weight: False\n",
            "Training layer1.2.bn2.bias: False\n",
            "Training layer1.2.conv3.weight: False\n",
            "Training layer1.2.bn3.weight: False\n",
            "Training layer1.2.bn3.bias: False\n",
            "Training layer2.0.conv1.weight: False\n",
            "Training layer2.0.bn1.weight: False\n",
            "Training layer2.0.bn1.bias: False\n",
            "Training layer2.0.conv2.weight: False\n",
            "Training layer2.0.bn2.weight: False\n",
            "Training layer2.0.bn2.bias: False\n",
            "Training layer2.0.conv3.weight: False\n",
            "Training layer2.0.bn3.weight: False\n",
            "Training layer2.0.bn3.bias: False\n",
            "Training layer2.0.downsample.0.weight: False\n",
            "Training layer2.0.downsample.1.weight: False\n",
            "Training layer2.0.downsample.1.bias: False\n",
            "Training layer2.1.conv1.weight: False\n",
            "Training layer2.1.bn1.weight: False\n",
            "Training layer2.1.bn1.bias: False\n",
            "Training layer2.1.conv2.weight: False\n",
            "Training layer2.1.bn2.weight: False\n",
            "Training layer2.1.bn2.bias: False\n",
            "Training layer2.1.conv3.weight: False\n",
            "Training layer2.1.bn3.weight: False\n",
            "Training layer2.1.bn3.bias: False\n",
            "Training layer2.2.conv1.weight: False\n",
            "Training layer2.2.bn1.weight: False\n",
            "Training layer2.2.bn1.bias: False\n",
            "Training layer2.2.conv2.weight: False\n",
            "Training layer2.2.bn2.weight: False\n",
            "Training layer2.2.bn2.bias: False\n",
            "Training layer2.2.conv3.weight: False\n",
            "Training layer2.2.bn3.weight: False\n",
            "Training layer2.2.bn3.bias: False\n",
            "Training layer2.3.conv1.weight: False\n",
            "Training layer2.3.bn1.weight: False\n",
            "Training layer2.3.bn1.bias: False\n",
            "Training layer2.3.conv2.weight: False\n",
            "Training layer2.3.bn2.weight: False\n",
            "Training layer2.3.bn2.bias: False\n",
            "Training layer2.3.conv3.weight: False\n",
            "Training layer2.3.bn3.weight: False\n",
            "Training layer2.3.bn3.bias: False\n",
            "Training layer3.0.conv1.weight: True\n",
            "Training layer3.0.bn1.weight: True\n",
            "Training layer3.0.bn1.bias: True\n",
            "Training layer3.0.conv2.weight: True\n",
            "Training layer3.0.bn2.weight: True\n",
            "Training layer3.0.bn2.bias: True\n",
            "Training layer3.0.conv3.weight: True\n",
            "Training layer3.0.bn3.weight: True\n",
            "Training layer3.0.bn3.bias: True\n",
            "Training layer3.0.downsample.0.weight: True\n",
            "Training layer3.0.downsample.1.weight: True\n",
            "Training layer3.0.downsample.1.bias: True\n",
            "Training layer3.1.conv1.weight: True\n",
            "Training layer3.1.bn1.weight: True\n",
            "Training layer3.1.bn1.bias: True\n",
            "Training layer3.1.conv2.weight: True\n",
            "Training layer3.1.bn2.weight: True\n",
            "Training layer3.1.bn2.bias: True\n",
            "Training layer3.1.conv3.weight: True\n",
            "Training layer3.1.bn3.weight: True\n",
            "Training layer3.1.bn3.bias: True\n",
            "Training layer3.2.conv1.weight: True\n",
            "Training layer3.2.bn1.weight: True\n",
            "Training layer3.2.bn1.bias: True\n",
            "Training layer3.2.conv2.weight: True\n",
            "Training layer3.2.bn2.weight: True\n",
            "Training layer3.2.bn2.bias: True\n",
            "Training layer3.2.conv3.weight: True\n",
            "Training layer3.2.bn3.weight: True\n",
            "Training layer3.2.bn3.bias: True\n",
            "Training layer3.3.conv1.weight: True\n",
            "Training layer3.3.bn1.weight: True\n",
            "Training layer3.3.bn1.bias: True\n",
            "Training layer3.3.conv2.weight: True\n",
            "Training layer3.3.bn2.weight: True\n",
            "Training layer3.3.bn2.bias: True\n",
            "Training layer3.3.conv3.weight: True\n",
            "Training layer3.3.bn3.weight: True\n",
            "Training layer3.3.bn3.bias: True\n",
            "Training layer3.4.conv1.weight: True\n",
            "Training layer3.4.bn1.weight: True\n",
            "Training layer3.4.bn1.bias: True\n",
            "Training layer3.4.conv2.weight: True\n",
            "Training layer3.4.bn2.weight: True\n",
            "Training layer3.4.bn2.bias: True\n",
            "Training layer3.4.conv3.weight: True\n",
            "Training layer3.4.bn3.weight: True\n",
            "Training layer3.4.bn3.bias: True\n",
            "Training layer3.5.conv1.weight: True\n",
            "Training layer3.5.bn1.weight: True\n",
            "Training layer3.5.bn1.bias: True\n",
            "Training layer3.5.conv2.weight: True\n",
            "Training layer3.5.bn2.weight: True\n",
            "Training layer3.5.bn2.bias: True\n",
            "Training layer3.5.conv3.weight: True\n",
            "Training layer3.5.bn3.weight: True\n",
            "Training layer3.5.bn3.bias: True\n",
            "Training layer4.0.conv1.weight: True\n",
            "Training layer4.0.bn1.weight: True\n",
            "Training layer4.0.bn1.bias: True\n",
            "Training layer4.0.conv2.weight: True\n",
            "Training layer4.0.bn2.weight: True\n",
            "Training layer4.0.bn2.bias: True\n",
            "Training layer4.0.conv3.weight: True\n",
            "Training layer4.0.bn3.weight: True\n",
            "Training layer4.0.bn3.bias: True\n",
            "Training layer4.0.downsample.0.weight: True\n",
            "Training layer4.0.downsample.1.weight: True\n",
            "Training layer4.0.downsample.1.bias: True\n",
            "Training layer4.1.conv1.weight: True\n",
            "Training layer4.1.bn1.weight: True\n",
            "Training layer4.1.bn1.bias: True\n",
            "Training layer4.1.conv2.weight: True\n",
            "Training layer4.1.bn2.weight: True\n",
            "Training layer4.1.bn2.bias: True\n",
            "Training layer4.1.conv3.weight: True\n",
            "Training layer4.1.bn3.weight: True\n",
            "Training layer4.1.bn3.bias: True\n",
            "Training layer4.2.conv1.weight: True\n",
            "Training layer4.2.bn1.weight: True\n",
            "Training layer4.2.bn1.bias: True\n",
            "Training layer4.2.conv2.weight: True\n",
            "Training layer4.2.bn2.weight: True\n",
            "Training layer4.2.bn2.bias: True\n",
            "Training layer4.2.conv3.weight: True\n",
            "Training layer4.2.bn3.weight: True\n",
            "Training layer4.2.bn3.bias: True\n",
            "Training fc.weight: False\n",
            "Training fc.bias: False\n",
            "Number of classes: ['apple', 'banana', 'beetroot', 'bell pepper', 'cabbage', 'capsicum', 'carrot', 'cauliflower', 'chilli pepper', 'corn', 'cucumber', 'eggplant', 'garlic', 'ginger', 'grapes', 'jalepeno', 'kiwi', 'lemon', 'lettuce', 'mango', 'onion', 'orange', 'paprika', 'pear', 'peas', 'pineapple', 'pomegranate', 'potato', 'raddish', 'soy beans', 'spinach', 'sweetcorn', 'sweetpotato', 'tomato', 'turnip', 'watermelon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Prepare Training\n",
        "# ----------------------------\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore', '.*UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images',)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Move the model to the device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# ----------------------------\n",
        "# Train\n",
        "# ----------------------------\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model on the training set\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        # Move the data to the device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the training loss\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            # Move the data to the device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update the test loss and accuracy\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_acc += torch.sum(preds == labels.data)\n",
        "\n",
        "    # Print the training and test loss and accuracy\n",
        "    train_loss /= len(train_data)\n",
        "    test_loss /= len(test_data)\n",
        "    test_acc = test_acc.double() / len(test_data)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {train_loss:.4f} Test Loss: {test_loss:.4f} Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"fine-tuned-resnet.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC4BmBSKvD5g",
        "outputId": "f7c4bec0-deb4-4e20-cdba-c0ab29cdbed8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Train Loss: 3.3206 Test Loss: 2.7704 Test Acc: 0.6895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] Train Loss: 2.2590 Test Loss: 1.2260 Test Acc: 0.8348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] Train Loss: 1.1976 Test Loss: 0.5644 Test Acc: 0.8689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] Train Loss: 0.7526 Test Loss: 0.3609 Test Acc: 0.9031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] Train Loss: 0.5856 Test Loss: 0.2867 Test Acc: 0.9259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] Train Loss: 0.4615 Test Loss: 0.2366 Test Acc: 0.9288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] Train Loss: 0.3843 Test Loss: 0.2119 Test Acc: 0.9288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] Train Loss: 0.3370 Test Loss: 0.1895 Test Acc: 0.9402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] Train Loss: 0.2832 Test Loss: 0.1770 Test Acc: 0.9459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] Train Loss: 0.2508 Test Loss: 0.1602 Test Acc: 0.9487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All layers:\n",
        "Epoch [9/40] Train Loss: 0.2780 Test Loss: 0.1734 Test Acc: 0.9487\n",
        "Epoch [11/40] Train Loss: 0.2054 Test Loss: 0.1482 Test Acc: 0.9487\n",
        "\n",
        "Training only last layer:\n",
        "Epoch [15/40] Train Loss: 0.8744 Test Loss: 0.6045 Test Acc: 0.8832\n",
        "Epoch [16/40] Train Loss: 0.8436 Test Loss: 0.5920 Test Acc: 0.8917\n",
        "Epoch [17/40] Train Loss: 0.8195 Test Loss: 0.5519 Test Acc: 0.9060\n",
        "Epoch [18/40] Train Loss: 0.7959 Test Loss: 0.5375 Test Acc: 0.8974\n",
        "\n",
        "Training last sequential block and FC layer:\n",
        "Epoch [8/10] Train Loss: 0.4360 Test Loss: 0.2392 Test Acc: 0.9231\n",
        "Epoch [9/10] Train Loss: 0.3891 Test Loss: 0.2130 Test Acc: 0.9259\n",
        "Epoch [10/10] Train Loss: 0.3608 Test Loss: 0.2028 Test Acc: 0.9288\n",
        "\n",
        "Training last two sequential blocks and FC layer:\n",
        "Epoch [7/10] Train Loss: 0.3843 Test Loss: 0.2119 Test Acc: 0.9288\n",
        "Epoch [8/10] Train Loss: 0.3370 Test Loss: 0.1895 Test Acc: 0.9402\n",
        "Epoch [9/10] Train Loss: 0.2832 Test Loss: 0.1770 Test Acc: 0.9459\n",
        "Epoch [10/10] Train Loss: 0.2508 Test Loss: 0.1602 Test Acc: 0.9487\n",
        "\n",
        "\n",
        "--> Final model fine-tuned-resnet_fruits-vegetables.pt - Tuned last two sequential blocks\n",
        "\n"
      ],
      "metadata": {
        "id": "dLa-yA3rNWHF"
      }
    }
  ]
}